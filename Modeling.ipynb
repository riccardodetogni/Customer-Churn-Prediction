{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f568c81-5fa8-4431-9c56-4a67bfbd2c7e",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "885d0c60-41c4-46e6-b699-ebcc87ff15cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score, roc_curve, auc, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aac53ce6-be9a-484c-8e59-86bc45444513",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ad2c6-a9ce-4fcb-b92e-7bab09bfdf42",
   "metadata": {},
   "source": [
    "## Data Preprocess\n",
    "- We want to build a preprocessor for numerical and categorical features.\n",
    "- Some of the models would prefer onehot encoded categorical features, others just label encoded so we build two different processors.\n",
    "- We can think of a way of balancing the dataset such as synthetic augmentation or use models that are resilient to unbalanced dataset (Random Forests, Boosted models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d9a55f1-3751-4f76-94df-ebdb2a459a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.drop('Exited', axis=1), data['Exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a573b35d-aaa0-46b1-9f43-a1f4b678ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e2b7e-8aa3-4a65-be83-120380ae01b8",
   "metadata": {},
   "source": [
    "### Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "62ba12f8-9c34-43ba-9a10-dce8ea174566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipelines for numerical and categorical features\n",
    "numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance (EUR)', 'NumberOfProducts', 'EstimatedSalary']\n",
    "categorical_features = ['Country', 'Gender', 'Sentiment', 'EstimatedSalary_Category', 'CreditScore_Category', 'Age_Category', 'Balance (EUR)_Category']\n",
    "\n",
    "preprocessor_onehot = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "        ], remainder='passthrough')\n",
    "\n",
    "preprocessor_label = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OrdinalEncoder(), categorical_features)\n",
    "        ], remainder='passthrough')\n",
    "\n",
    "def create_pipeline(model, preprocessor, augment=False):\n",
    "    if augment:\n",
    "        return ImbPipeline(steps=[('preprocessor', preprocessor),\n",
    "                                  ('smote', SMOTE(random_state=42)),\n",
    "                                  ('classifier', model)])\n",
    "    else:\n",
    "        return Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                  ('classifier', model)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd6587-5356-4e02-b87a-f68f705f9ced",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "- In this section we try different models with different parameter space.\n",
    "- We create a method that allow us to introduce augmentation in the training data.\n",
    "- We select the best performing model in terms of recall since we are interested in getting all the churning clients (clients on which we want to focus our business decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f58af758-2a98-4e0a-8d84-f597900c8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models(augment=False):\n",
    "    models = [\n",
    "        {\n",
    "            'name': 'Logistic Regression',\n",
    "            'estimator': create_pipeline(LogisticRegression(solver='liblinear'), preprocessor_onehot, augment),\n",
    "            'params': {\n",
    "                'classifier__C': [0.1, 1.0, 10],\n",
    "                'classifier__penalty': ['l1', 'l2']\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Random Forest',\n",
    "            'estimator': create_pipeline(RandomForestClassifier(random_state=42), preprocessor_label, augment),\n",
    "            'params': {\n",
    "                'classifier__n_estimators': [100, 200, 300],\n",
    "                'classifier__max_depth': [None, 10, 20, 50],\n",
    "                'classifier__min_samples_split': [2, 5, 10],\n",
    "                'classifier__min_samples_leaf': [1, 2]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'XGBoost',\n",
    "            'estimator': create_pipeline(XGBClassifier(random_state=42), preprocessor_label, augment),\n",
    "            'params': {\n",
    "                'classifier__n_estimators': [200, 300, 500],\n",
    "                'classifier__learning_rate': [0.01, 0.1, 0.2, 0.25],\n",
    "                'classifier__max_depth': [3, 5, 7],\n",
    "                'classifier__gamma': [0, 0.1, 0.2]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "        'name': 'Support Vector Classifier',\n",
    "        'estimator': create_pipeline(SVC(probability=True, random_state=42), preprocessor_onehot, augment),\n",
    "        'params': {\n",
    "            'classifier__C': [1.0, 10, 50],\n",
    "            'classifier__kernel': ['rbf'],\n",
    "            'classifier__gamma': ['scale', 'auto']\n",
    "        }\n",
    "        }\n",
    "    ]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "200f197e-a086-4e1e-a75d-88197c333869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scoring(models):\n",
    "    best_models = []\n",
    "    for model in models:\n",
    "        print(f\"Training {model['name']}...\")\n",
    "        grid_search = GridSearchCV(model['estimator'], model['params'], cv=5, n_jobs=-1, scoring='recall')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_models.append({\n",
    "            'name': model['name'],\n",
    "            'best_estimator': grid_search.best_estimator_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_\n",
    "        })\n",
    "        print(f\"Best parameters for {model['name']}: {grid_search.best_params_}\")\n",
    "        print(f\"Best cross-validation recall score for {model['name']}: {grid_search.best_score_}\")\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b5a2492-116e-42e0-ae94-92fafb43b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(best_models, X_test, y_test):\n",
    "    for model in best_models:\n",
    "        print(f\"Evaluating {model['name']}...\")\n",
    "        y_pred = model['best_estimator'].predict(X_test)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        print(f\"Test Recall for {model['name']}: {recall}\")\n",
    "        print(f\"Confusion Matrix for {model['name']}:\\n{conf_matrix}\")\n",
    "        print(f\"Classification Report for {model['name']}:\\n{class_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f40fd9e-27b1-4050-ba1b-c017e8e3f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = build_models()\n",
    "models_augmented = build_models(augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339cfe1-b895-4cd7-b143-7955c1321cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = model_scoring(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e3ffa1c-7349-4bcf-8f55-38f5e8c6f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Best parameters for Logistic Regression: {'classifier__C': 0.1, 'classifier__penalty': 'l2'}\n",
      "Best cross-validation recall score for Logistic Regression: 0.7061349693251534\n",
      "Training Random Forest...\n",
      "Best parameters for Random Forest: {'classifier__max_depth': 10, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 500}\n",
      "Best cross-validation recall score for Random Forest: 0.592638036809816\n",
      "Training XGBoost...\n",
      "Best parameters for XGBoost: {'classifier__gamma': 0, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 200}\n",
      "Best cross-validation recall score for XGBoost: 0.6306748466257669\n"
     ]
    }
   ],
   "source": [
    "best_models_aug = model_scoring(models_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cff4ae77-99a7-410c-a591-cb22151df0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Logistic Regression...\n",
      "Test Recall for Logistic Regression: 0.32432432432432434\n",
      "Confusion Matrix for Logistic Regression:\n",
      "[[1533   60]\n",
      " [ 275  132]]\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90      1593\n",
      "           1       0.69      0.32      0.44       407\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.77      0.64      0.67      2000\n",
      "weighted avg       0.82      0.83      0.81      2000\n",
      "\n",
      "Evaluating Random Forest...\n",
      "Test Recall for Random Forest: 0.4742014742014742\n",
      "Confusion Matrix for Random Forest:\n",
      "[[1545   48]\n",
      " [ 214  193]]\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92      1593\n",
      "           1       0.80      0.47      0.60       407\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.84      0.72      0.76      2000\n",
      "weighted avg       0.86      0.87      0.86      2000\n",
      "\n",
      "Evaluating XGBoost...\n",
      "Test Recall for XGBoost: 0.47174447174447176\n",
      "Confusion Matrix for XGBoost:\n",
      "[[1531   62]\n",
      " [ 215  192]]\n",
      "Classification Report for XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92      1593\n",
      "           1       0.76      0.47      0.58       407\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.82      0.72      0.75      2000\n",
      "weighted avg       0.85      0.86      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_models(best_models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5f41eee4-572d-4131-8e80-79caa5136e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Logistic Regression...\n",
      "Test Recall for Logistic Regression: 0.6928746928746928\n",
      "Confusion Matrix for Logistic Regression:\n",
      "[[1209  384]\n",
      " [ 125  282]]\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.76      0.83      1593\n",
      "           1       0.42      0.69      0.53       407\n",
      "\n",
      "    accuracy                           0.75      2000\n",
      "   macro avg       0.66      0.73      0.68      2000\n",
      "weighted avg       0.81      0.75      0.76      2000\n",
      "\n",
      "Evaluating Random Forest...\n",
      "Test Recall for Random Forest: 0.6044226044226044\n",
      "Confusion Matrix for Random Forest:\n",
      "[[1477  116]\n",
      " [ 161  246]]\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91      1593\n",
      "           1       0.68      0.60      0.64       407\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.79      0.77      0.78      2000\n",
      "weighted avg       0.86      0.86      0.86      2000\n",
      "\n",
      "Evaluating XGBoost...\n",
      "Test Recall for XGBoost: 0.597051597051597\n",
      "Confusion Matrix for XGBoost:\n",
      "[[1438  155]\n",
      " [ 164  243]]\n",
      "Classification Report for XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90      1593\n",
      "           1       0.61      0.60      0.60       407\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.75      0.75      0.75      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_models(best_models_aug, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d9708-d645-47f1-8b1d-c8053a897cdd",
   "metadata": {},
   "source": [
    "## Comments on model selection\n",
    "- It seems that with not augmented training dataset the performance of the models are worse, in terms of positive class recall.\n",
    "- The best model seems to be the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2365c696-e1b2-4cb9-96cf-5905b553e48c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
